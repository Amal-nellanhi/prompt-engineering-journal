# 🔬 Prompt Engineering Lab Journal

### A Systematic, 9-Part Study on Generative AI Instruction Effectiveness

---

## 🎯 Project Objective

This journal serves as a personal laboratory to meticulously design, test, and evaluate various prompt structures (Chain-of-Thought, Role-based, Zero-shot vs. Few-shot, etc.) to understand how specific variables—such as clarity, tone, and constraints—directly impact the quality, accuracy, and creativity of outputs from leading AI models.

## 🔗 Live Documentation Site

The full, interactive documentation, including all project findings, data visualizations, and scoring rubrics, is hosted live via GitHub Pages:

➡️ **[VIEW THE LIVE JOURNAL HERE](https://Amal-nellanhi.github.io/prompt-engineering-journal/)**


---

## 📂 Repository Structure

| Folder/File | Purpose |
| :--- | :--- |
| `docs/` | Contains all project content written in **Markdown** (`project-1.md`, `index.md`). |
| `mkdocs.yml` | The primary configuration file, defining site title, theme, and navigation. |
| `site/` | **(IGNORED)** The directory generated by `mkdocs build`—the final HTML website. |
| `README.md` | This overview and project summary. |

---

## 🛠️ Tech Stack & Methodology

- **Documentation:** Markdown
- **Static Site Generator:** [MkDocs](https://www.mkdocs.org/) (using the Material theme)
- **Deployment:** GitHub Pages
- **Models Tested:** Gemini (2.5 Flash), ChatGPT (GPT-4/GPT-4o), Claude (Sonnet 4.5), Perplexity AI

---

## 📜 License

This work is protected under the **All Rights Reserved** License. Please see the LICENSE file for details.
